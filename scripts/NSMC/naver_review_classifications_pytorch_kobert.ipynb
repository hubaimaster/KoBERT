{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Requirement already satisfied: mxnet in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (1.8.0.post0)\r\nRequirement already satisfied: numpy<2.0.0,>1.16.0 in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (from mxnet) (1.21.2)\r\nRequirement already satisfied: requests<3,>=2.20.0 in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (from mxnet) (2.26.0)\r\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (from mxnet) (0.8.4)\r\nRequirement already satisfied: certifi>=2017.4.17 in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (from requests<3,>=2.20.0->mxnet) (2021.5.30)\r\nRequirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (from requests<3,>=2.20.0->mxnet) (2.0.6)\r\nRequirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (from requests<3,>=2.20.0->mxnet) (3.2)\r\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (from requests<3,>=2.20.0->mxnet) (1.26.7)\r\n",
      "Requirement already satisfied: gluonnlp in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (0.10.0)\r\nRequirement already satisfied: pandas in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (1.3.3)\r\nRequirement already satisfied: tqdm in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (4.62.3)\r\nRequirement already satisfied: numpy>=1.16.0 in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (from gluonnlp) (1.21.2)\r\n",
      "Requirement already satisfied: cython in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (from gluonnlp) (0.29.24)\r\nRequirement already satisfied: packaging in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (from gluonnlp) (21.0)\r\nRequirement already satisfied: pytz>=2017.3 in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (from pandas) (2021.3)\r\nRequirement already satisfied: python-dateutil>=2.7.3 in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (from pandas) (2.8.2)\r\nRequirement already satisfied: pyparsing>=2.0.2 in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (from packaging->gluonnlp) (2.4.7)\r\nRequirement already satisfied: six>=1.5 in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\r\n",
      "Requirement already satisfied: sentencepiece in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (0.1.91)\r\n",
      "Requirement already satisfied: transformers in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (3.5.1)\r\n",
      "Requirement already satisfied: sentencepiece==0.1.91 in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (from transformers) (0.1.91)\r\nRequirement already satisfied: protobuf in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (from transformers) (3.18.0)\r\nRequirement already satisfied: packaging in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (from transformers) (21.0)\r\nRequirement already satisfied: numpy in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (from transformers) (1.21.2)\r\nRequirement already satisfied: tokenizers==0.9.3 in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (from transformers) (0.9.3)\r\nRequirement already satisfied: tqdm>=4.27 in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (from transformers) (4.62.3)\r\nRequirement already satisfied: sacremoses in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (from transformers) (0.0.46)\r\nRequirement already satisfied: filelock in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (from transformers) (3.3.0)\r\nRequirement already satisfied: requests in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (from transformers) (2.26.0)\r\nRequirement already satisfied: regex!=2019.12.17 in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (from transformers) (2021.9.30)\r\nRequirement already satisfied: pyparsing>=2.0.2 in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (from packaging->transformers) (2.4.7)\r\nRequirement already satisfied: click in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (from sacremoses->transformers) (8.0.1)\r\n",
      "Requirement already satisfied: joblib in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.1)\r\nRequirement already satisfied: six in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (from sacremoses->transformers) (1.16.0)\r\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\r\nRequirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (from requests->transformers) (2.0.6)\r\nRequirement already satisfied: certifi>=2017.4.17 in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (from requests->transformers) (2021.5.30)\r\nRequirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (from requests->transformers) (3.2)\r\n",
      "Requirement already satisfied: torch in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (1.9.1)\r\nRequirement already satisfied: typing-extensions in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages (from torch) (3.10.0.2)\r\n",
      "Collecting git+https://git@github.com/SKTBrain/KoBERT.git@master\r\n",
      "  Cloning https://git@github.com/SKTBrain/KoBERT.git (to revision master) to /private/var/folders/m6/q5txfd0x5zzcp32y_s1spr4m0000gn/T/pip-req-build-f51u0s6p\r\n",
      "Requirement already satisfied (use --upgrade to upgrade): kobert==0.1.2 from git+https://git@github.com/SKTBrain/KoBERT.git@master in /Users/changhwankim/venv/KoBERT_Whatssub38/lib/python3.8/site-packages\r\n",
      "using cached model\n",
      "using cached model\n",
      "zsh:1: no matches found: https://www.dropbox.com/s/374ftkec978br3d/ratings_train.txt?dl=1\r\n",
      "zsh:1: no matches found: https://www.dropbox.com/s/977gbwh542gdy94/ratings_test.txt?dl=1\r\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m6/q5txfd0x5zzcp32y_s1spr4m0000gn/T/ipykernel_10953/2967657600.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wget https://www.dropbox.com/s/977gbwh542gdy94/ratings_test.txt?dl=1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mdataset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTSVDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ratings_train.txt?dl=1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_discard_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mdataset_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTSVDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ratings_test.txt?dl=1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_discard_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/KoBERT_Whatssub38/lib/python3.8/site-packages/gluonnlp/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, encoding, sample_splitter, field_separator, num_discard_samples, field_indices, allow_missing)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_field_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfield_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_allow_missing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mallow_missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTSVDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_should_discard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/KoBERT_Whatssub38/lib/python3.8/site-packages/gluonnlp/data/dataset.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mall_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filenames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encoding\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m                 \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample_splitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_discard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ratings_train.txt?dl=1'"
     ],
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ratings_train.txt?dl=1'",
     "output_type": "error"
    }
   ],
   "source": [
    "!pip install mxnet\n",
    "!pip install gluonnlp pandas tqdm\n",
    "!pip install sentencepiece\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "\n",
    "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "##GPU 사용 시\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "bertmodel, vocab = get_pytorch_kobert_model()\n",
    "\n",
    "# !wget https://www.dropbox.com/s/374ftkec978br3d/ratings_train.txt?dl=1\n",
    "# !wget https://www.dropbox.com/s/977gbwh542gdy94/ratings_test.txt?dl=1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "using cached model\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "dataset_train = nlp.data.TSVDataset(\"ratings_train.txt\", field_indices=[1,2], num_discard_samples=1)\n",
    "dataset_test = nlp.data.TSVDataset(\"ratings_test.txt\", field_indices=[1,2], num_discard_samples=1)\n",
    "\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n",
    "\n",
    "\n",
    "## Setting parameters\n",
    "max_len = 64\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=2,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)\n",
    "\n",
    "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n",
    "\n",
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}